---
title: "Coursework assignment A - 2022-2023"
subtitle: "CS4125 Seminar Research Methodology for Data Science"
author: "Student names"
date: "16/04/2023"
output:
   pdf_document:
      fig_caption: true
      number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\tableofcontents


# Part 1 - Design and set-up of true experiment 


## The motivation for the planned research 
(Max 200 words)

## The theory underlying the research  
(Max 200 words) Preferable based on theories reported in literature


## Research questions 
The research question that will be examined in an experiment (or alternatively the hypothesis that will be tested in an  experiment)


## The related conceptual model 
This model should include:
*Independent variable(s)
*Dependent variable
*Mediating variable (at least 1)
*Moderating variable (at least 1)


## Experimental Design 
Experimental Design (the study should have a true experimental design to test a single hypothesis that, for simplicity, includes only independent variable(s) and dependent variable(s). In other words, mediating and moderating variables are not included in the experimental design ) 

## Experimental procedure 
Describe how the experiment will be executed step by step


## Measures
Describe the measure that will be used

## Participants
Describe which participants will recruit in the study and how they will be recruited

## Suggested statistical analyses
Describe the statistical test you suggest to care out on the collected data

# Part 2 - Generalized linear models

## Question 1 Twitter sentiment analysis (Between groups - single factor) 

### Conceptual model
Make a conceptual model for the following research question: Is there a difference in the sentiment of the tweets related to the different individuals/organisations?

### Model description

Describe the mathematical model fitted on the most extensive model. (hint, look at the mark down file of the lectures to see example on formulate mathematical models in markdown). Assume a Gaussian distribution for the tweet’s sentiments rating. Justify the priors.

### Generate Synthetic data
Create a synthetic data set with a clear difference between tweets’ sentiments of celebrities for verifying your analysis later on. Report the values of the coefficients of the linear model used to generate synthetic data. (hint, look at class lecture slides of lecture on Generalized linear models for example to create synthetic data)


```{r}
#include your code for generating the synthetic data
```

### Collecting tweets, and data preparation
Include the annotated R script (excluding your personal Keys and Access Tokens information), but put echo=FALSE, so code is not included in the output pdf file.


```{r, echo=FALSE, message=FALSE, warning=FALSE, include = FALSE}

#during writing you could add "eval = FALSE",  kntr will than not run this code chunk (take some time do)

setwd("~/surfdrive/Teaching/own teaching/IN4125 - Seminar Research Methodology for Data Science/2019/coursework A") 
# apple , note use / instead of \, which used by windows


#install.packages("twitteR", dependencies = TRUE)
library(twitteR)
#install.packages("RCurl", dependencies = T)
library(RCurl)
#install.packages("bitops", dependencies = T)
library(bitops)
#install.packages("plyr", dependencies = T)
library(plyr)
#install.packages('stringr', dependencies = T)
library(stringr)
#install.packages("NLP", dependencies = T)
library(NLP)
#install.packages("tm", dependencies = T)
library(tm)
#install.packages("wordcloud", dependencies=T)
#install.packages("RColorBrewer", dependencies=TRUE)
library(RColorBrewer)
library(wordcloud)
#install.packages("reshape", dependencies=T)
library(reshape)

################### functions

  
clearTweets <- function(tweets, excl) {
  
  tweets.text <- sapply(tweets, function(t)t$getText()) #get text out of tweets 

  
  tweets.text = gsub('[[:cntrl:]]', '', tweets.text)
  tweets.text = gsub('\\d+', '', tweets.text)
  tweets.text <- str_replace_all(tweets.text,"[^[:graph:]]", " ") #remove graphic
  
  
  corpus <- Corpus(VectorSource(tweets.text))
  
  corpus_clean <- tm_map(corpus, removePunctuation)
  corpus_clean <- tm_map(corpus_clean, content_transformer(tolower))
  corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("english"))
  corpus_clean <- tm_map(corpus_clean, removeNumbers)
  corpus_clean <- tm_map(corpus_clean, stripWhitespace)
  corpus_clean <- tm_map(corpus_clean, removeWords, c(excl,"http","https","httpst"))
  

  return(corpus_clean)
} 


## capture all the output to a file.

################# Collect from Twitter

# for creating a twitter app (apps.twitter.com) see youtube https://youtu.be/lT4Kosc_ers
#consumer_key <-'your key'
#consumer_scret <- 'your secret'
#access_token <- 'your access token'
#access_scret <- 'your access scret'

source("wpb_twitter.R") #this file will set my personal variables for my twitter app, adjust the name of this file. use the provide template your_twitter.R

setup_twitter_oauth(consumer_key,consumer_scret, access_token,access_scret) #connect to  twitter app


##### This example uses the following 3 celebrities: Donald Trump, Hillary Clinton, and Bernie Sanders
##  You should replace this with your own celebrities, at least 3, but more preferred 
##  Note that it will take the computer some to collect the tweets

tweets_T <- searchTwitter("#trump", n=100, lang="en", resultType="recent") #n recent tweets about Donald Trump, in English ( Twitter sometimes modifies number of tweets that you can collect)
tweets_C <- searchTwitter("#hillary", n=100, lang="en", resultType="recent") #n recent tweets about Hillary Clinton
tweets_B <- searchTwitter("#bernie", n=100, lang="en", resultType="recent") #n recent tweets about Bernie Sanders

#overtime Twitter allow fewer tweets to be collected so you might have to adjust this number

######################## WordCloud
### This not requires in the assignment, but still fun to do 

# based on https://youtu.be/JoArGkOpeU0

#corpus_T<-clearTweets(tweets_T, c("trump","amp","realdonaldtrump","trumptrain","donald","trumps","alwaystrump")) #remove also some campain slogans
#wordcloud(corpus_T, max.words=50)

#corpus_C<-clearTweets(tweets_C, c("hillary","amp","clinton","hillarys"))
#wordcloud(corpus_C,  max.words=50)

#corpus_B<-clearTweets(tweets_B, c("bernie", "amp", "sanders","bernies"))
#wordcloud(corpus_B,  max.words=50)
##############################

######################## Sentiment analysis

tweets_T.text <- laply(tweets_T, function(t)t$getText()) #get text out of tweets 
tweets_C.text <- laply(tweets_C, function(t)t$getText()) #get text out of tweets
tweets_B.text <- laply(tweets_B, function(t)t$getText()) #get text out of tweets



#taken from https://github.com/mjhea0/twitter-sentiment-analysis
pos <- scan('positive-words.txt', what = 'character', comment.char=';') #read the positive words
neg <- scan('negative-words.txt', what = 'character', comment.char=';') #read the negative words

source("sentiment3.R") #load algorithm
# see sentiment3.R form more information about sentiment analysis. It assigns a intereger score
# by subtracting the number of occurrence of negative words from that of positive words

analysis_T <- score.sentiment(tweets_T.text, pos, neg)
analysis_C <- score.sentiment(tweets_C.text, pos, neg)
analysis_B <- score.sentiment(tweets_B.text, pos, neg)


sem<-data.frame(analysis_T$score, analysis_C$score, analysis_B$score)


semFrame <-melt(sem, measured=c(analysis_T.score,analysis_C.score, analysis_B.score ))
names(semFrame) <- c("Candidate", "score")
semFrame$Candidate <-factor(semFrame$Candidate, labels=c("Donald Trump", "Hillary Clinton", "Bernie Sanders")) # change the labels for your individual/organisation

#The data you need for the analyses can be found in semFrame

```


### Visual inspection Mean and distribution sentiments
Graphically examine the mean and distribution sentiments of tweets for each individual/organisation, and provide interpretation

```{r}
#include your analysis code and output in the document
```

### Frequentist approach

#### Analysis verification
Verify your model analysis with synthetic data and show that it can reproduce the coefficients of the linear model that you used to generate the synthetic data set. Provide a short interpretation of the results, with a reflection of AICc, F-value, p-value etc.

```{r}
#include your analysis code of synthetic data and output in the document
```


#### Linear model
Redo the analysis now on the real tweet data set. Provide a short interpretation of the results, with an interpretation of AICc, F-value, p-value, etc.

```{r}
#include your analysis code and output in the document
```

#### Post Hoc analysis
If a model that includes the individual better explains the sentiments of tweets than a model without such predictor, conduct a posthoc analysis with, e.g., Bonferroni correction to examine which celebrity tweets differ from the other individual’s tweets. Provide a brief interpretation of the results.

```{r}
#include your code and output in the document
```

#### Report section for a scientific publication
Write a small section for a scientific publication (journal or a conference), in which you report the results of the analyses, and explain the conclusions that can be drawn in a format commonly used by the scientific community Look at Brightspace for examples papers and guidelines on how to do this. (Hint, there are strict guidelines for reporting statistical results in paper, I expect you to follow these here) 

### Bayesian Approach
For the Bayesian analyses, use the rethinking and/or BayesianFirstAid library
#### Analysis verification
Verify your model analysis with synthetic data and show that it can reproduce the coefficients of the linear model that you used to generate the synthetic data set. Provide a short interpretation of the results, with a reflection of WAIC, and 95% credibility interval of coefficients for individual celebrities.

```{r}
#include your analysis code of synthetic data and output in the document
```

#### Model comparison
Redo the analysis on the actual tweet data set. Provide a short interpretation of the results, with a reflection of WAIC, and 95% credibility interval of coefficients for individual celebrities.

```{r}
#include your code and output in the document
```

#### Comparison individual/organisation pair
Compare sentiments of individual pairs and provide a brief interpretation (e.g. CIs) 

```{r}
#include your code and output in the document
```

## Question 2 - Website visits (between groups - Two factors)

### Conceptual model
Make a conceptual model underlying this research question

![](CM.jpg)

### Specific Mathematical model
Describe the mathematical model that you fit on the data. Take for this the complete model that you fit on the data. Also, explain your selection for the priors. Assume Gaussian distribution for the number of page visits.



I adopt linear regression model o fit the data, which can be expressed as follows:

y = β₀ + β₁ * version + β₂ * portal + β₃ * version * portal + ε



- y represents the number of page visits, which is assumed to follow a Gaussian distribution.
- version is a binary variable (0 for the old version, 1 for the new version) indicating the website version.
- portal is a binary variable (0 for consumers, 1 for companies) indicating the web portal entry.
- version * portal represents the interaction term between the website version and portal.
- β₀ is intercept and β₁, β₂, and β₃ are the coefficients for the three terms. I adopt Cauchy distribution for them assuming weakly informative prior, which has a large scale value has a gentle slope, letting data in the more extreme region still be of influence if the likelihood is strong here
- ε represents the error term, assumed to follow a Gaussian distribution, ε∼ Normal(0, σ).

### Create Synthetic data
Create a synthetic data set with a clear interaction effect between the two factors for verifying your analysis later on. Report the values of the coefficients of the linear model used to generate synthetic data.

```{r}
#include your code for generating the synthetic data

# Set the seed for reproducibility
set.seed(1)

# Specify the sample size
n <- 100

# Create the independent variables
version <- rep(c(0, 1), each = n/2)
portal <- rep(c(0, 1), times = n/2)

# Generate the interaction effect
interaction <- version * portal

# the values of the coefficients of the linear model
beta0 <- 2.5     # Intercept
beta1 <- 1.5     # Coefficient for version
beta2 <- 0.8     # Coefficient for portal
beta3 <- 0.7     # Coefficient for interaction

# Generate the dependent variable (number of page visits)
page_visits <- beta0 + betbetaa1 * version + beta2 * portal + beta3 * interaction + rnorm(n)

# Combine the variables into a data frame
data <- data.frame(version, portal, interaction, page_visits)

# View the first few rows of the synthetic data set
head(data)

```


### Visual inspection
Graphically examine the mean page visits for the four different conditions. Give a short explanation of the figure.


```{r}
#include your code and output in the document

# Load the required library
library(ggplot2)

# Compute the mean page visits for each condition
mean_data <- aggregate(page_visits ~ version + portal, data, mean)

# Create a grouped bar plot
ggplot(mean_data, aes(x = version, y = page_visits, fill = factor(portal))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Website Version", y = "Mean Page Visits") +
  scale_fill_discrete(name = "Portal") +
  theme_bw()
```

![](visual_inspection_2.jpg)

I conducted a simple effect analysis to examine the influence of one independent variable on different levels of another independent variable. The analysis revealed that the combination of the new version and web portal for companies resulted in a highest number of page visits. Furthermore, regardless of the website version, the portal for companies showed a higher number of page visits compared to the portal for consumers. Additionally, irrespective of the portal, the old version exhibited fewer page visits compared to the new version.



### Frequentist Approach

#### Model verification
Verify your model analysis with synthetic data and show that it can reproduce the coefficients of the linear model that you used to generate the synthetic data set. Provide a short interpretation of the results, with a reflection of AICc, F-value, p-value etc.

```{r}
#include your analysis code of synthetic data and output in the document

# Fit the linear regression model
model <- lm(page_visits ~ version + portal + interaction, data = data)

# Print the model summary
summary(model)

# Calculate AIC
AIC <- AIC(model)

# Calculate the number of parameters
k <- length(coef(model))

# Calculate the AICc
n <- nrow(data)
AICc <- AIC + (2 * k * (k + 1)) / (n - k - 1)


# Print the results
cat("Coefficients:\n")
print(coef(model))

cat("\nAIC:", AIC)
cat("\nAICc:", AICc)


#result:

#Coefficients:
#            Estimate Std. Error t value Pr(>|t|)    
#(Intercept)   2.6961     0.1815  14.850  < 2e-16 ***
#version       1.4989     0.2567   5.838 7.16e-08 ***
#portal        0.6088     0.2567   2.371   0.0197 *  
#interaction   0.7359     0.3631   2.027   0.0455 *  

#F-statistic: 46.26 on 3 and 96 DF,  p-value: < 2.2e-16

#AIC: 270.3466

#AICc: 270.7676

#interpretation
#1. All coefficients are comparable to original values and statistically significant.
#2. The p-value is reported as "< 2.2e-16", which is essentially zero. This extremely low p-value indicates strong evidence against the null hypothesis, suggesting that the predictors collectively have a significant effect on the outcome variable (page visits).
#3. The AICc value of 270.3466 suggests that the fitted linear regression model has a relatively good fit to the data compared to alternative models.
```

#### Model analysis with Gaussian distribution assumed
Redo the analysis now on the real data set. Assume Gaussian distribution for the number of page visits. Provide a short interpretation of the results, with an interpretation of AICc, F-value, p-value, etc.


```{r}
#include your code and output in the document

#include your analysis code of synthetic data and output in the document

#set work path
setwd("D:/seminar/assignment")

#read data and add interaction
web_data <- read.csv("webvisit0.csv") 
web_data$interaction <- web_data$version * web_data$portal

# Fit the linear regression model
model0 <- lm(pages ~ version + portal + interaction, data = web_data)

# Print the model summary
summary(model0)


#result:

#Coefficients:
#            Estimate Std. Error t value Pr(>|t|)    
#(Intercept)  19.6280     0.3443   57.02   <2e-16 ***
#version      -7.6239     0.4898  -15.56   <2e-16 ***
#portal       13.4663     0.4898   27.49   <2e-16 ***
#interaction  29.8808     0.6888   43.38   <2e-16 ***

#F-statistic:  3110 on 3 and 996 DF,  p-value: < 2.2e-16

#interpretation
#1. All coefficients are statistically significant， which show that version, portal and interaction have a strong influence on the page visit.
#2. The p-value is reported as "< 2.2e-16", which is essentially zero. This extremely low p-value indicates strong evidence against the null hypothesis, suggesting that the predictors collectively have a significant effect on the outcome variable (page visits).
```

#### Assumption analysis
Redo the analysis on the real tweet data set.  This time assume a Poisson distribution for the number of page visits. For the best fitting models (Gaussian and Poisson), examine graphically the distribution of the residuals for the model that assumes Gaussian distribution and the model that assumes Poisson distribution. Give a brief interpretation of Poisson and Gaussian distribution assumptions.

```{r}
#include your code and output in the document

# Poisson regression
poisson_model <- glm(pages ~ version + portal + interaction, data = web_data, family = poisson)

# Gaussian regression
gaussian_model <- model0

# Residual analysis
poisson_residuals <- resid(poisson_model)
gaussian_residuals <- resid(gaussian_model)

# Histogram of Poisson model residuals
hist(poisson_residuals, breaks = 30, col = "lightblue", main = "Poisson Model Residuals")

# Density plot of Gaussian model residuals
hist(gaussian_residuals, breaks = 30, col = "lightblue", main = "Gaussian Model Residuals")
```

![](gaussian.jpg)

![poisson](poisson.jpg)

Both the Gaussian and Poisson residuals appear to follow a normal distribution, which suggests that the Gaussian model is a better fit for the data.

#### Simple effect analysis

Continue with the model that assumes a Poisson distribution. If the analysis shows a significant two-way interaction effect, conduct a Simple Effect analysis to explore this interaction effect in more detail. Provide a brief interpretation of the results.


```{r}
#include your code and output in the document

library(rethinking)
summary(poisson_model)
# interaction  1.00605    0.02717   37.03   <2e-16 ***
# The p-value is reported as "< 2.2e-16", which is essentially zero. This extremely low p-value indicates strong evidence against the null hypothesis, suggesting that the interaction have a significant effect on the page visits.

library(pander)

# create two contrasts and combine them and associate the contrast to a variable
web_data$simple <- interaction(web_data$version, web_data$portal) #merge two factors
levels(web_data$simple) #to see the level in the new factor

contrastOld <-c(1,-1,0,0) 
contrastNew <-c(0,0,1,-1) 

SimpleEff <- cbind(contrastOld,contrastNew)
contrasts(web_data$simple) <- SimpleEff #now we link the two contrasts with the version

# we fit a linear model on the data, using this two-level variable as an independent factor.
simpleEffectModel <-lm(pages ~ simple , data = web_data, na.action = na.exclude)
pander(summary.lm(simpleEffectModel))

# result:
#----------------------------------------------------------------------
#        &nbsp;           Estimate   Std. Error   t value    Pr(>|t|)  
#----------------------- ---------- ------------ --------- ------------
#    **(Intercept)**       30.02       0.1722      174.3        0      
#
# **simplecontrastOld**    3.812       0.2449      15.56    4.691e-49  

# **simplecontrastNew**    -11.13      0.2421     -45.96    2.195e-248 

#      **simple**          28.41       0.3444      82.48        0      
#----------------------------------------------------------------------

# It revealed a significant (t = 15.56, p. < 0.01) difference for old version in page visits, and also a significant effect (t = -45.96,p. < 0.01 ) was found for the onew version in page visits.
```


#### Report section for a scientific publication
Write a small section for a scientific publication, in which you report the results of the analyses, and explain the conclusions that can be drawn.



Paper: [Effects of different real-time feedback types on human performance in high-demanding work conditions](https://www.sciencedirect.com/science/article/pii/S1071581916000392)

Result:

![](table.jpg)



The table shows the simple effect of HR, performance and error as well as the 2-way/3-way interaction between/among them. We can see from the table that there is a significant two-way interaction effect between HR and Error.(p<0.002)  Also, the HR itself is a significant effect(p<0,006).

### Bayesian Approach
For the Bayesian analyses, use the rethinking and/or BayesianFirstAid library

####  Verification Analysis
Verify your model analysis with synthetic data and show that it can reproduce the coefficients of the linear model that you used to generate the synthetic data set. Provide a short interpretation of the results, with a reflection of WAIC, and 95% credibility interval of coefficients for individual celebrities.


```{r}
#include your analysis code of synthetic data and output in the document
library(rethinking)

model_bay_fake <- map(
  alist(
    page_visits ~ dnorm(mu, sigma),
    mu <- beta0 + beta1*version + beta2*portal + beta3*interaction,
    beta0 ~ dnorm(0, 10),
    beta1 ~ dnorm(0, 10),
    beta2 ~ dnorm(0, 10),
    beta3 ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 2.5)
  ),
  data = data,
  start = list(beta0 = 2.5, beta1 = 1.5, beta2 = 0.8, beta3 = 0.7, sigma = 1)
)


precis(model_bay_fake, prob = .95)

waic <- WAIC(model_bay_fake)
waic

#Result
#      mean   sd 2.5% 97.5%
#beta0 2.70 0.18 2.35  3.04
#beta1 1.50 0.25 1.01  1.99
#beta2 0.61 0.25 0.12  1.10
#beta3 0.74 0.36 0.04  1.43
#sigma 0.89 0.06 0.77  1.01

#      WAIC      lppd  penalty  std_err
#1 271.2522 -130.2396 5.386505 15.16342

# The estimated coefficients of the synthetic data closely match the original coefficients used to generate the data, with portal, version and interaction all positively affect the page visit.
```


#### Model description

Describe the mathematical model fitted on the most extensive model. (hint, look at the mark down file of the lectures to see example on formulate mathematical models in markdown). Assume Poisson distribution for the number of page visits. Justify the priors.



Model:

Pages ~ Poisson(lambda) 

lambda = exp(beta0 + beta1 * Version + beta2 * Portal + beta3 * Interaction)



Priors:

Because there is limited prior information or no strong prior beliefs, I use weakly informative priors  that allow the data to have a larger influence on the posterior results.

- beta0 ~ Normal(0, 10) 
- beta1 ~ Normal(0, 10)
- beta2 ~ Normal(0, 10)
- beta3 ~ Normal(0, 10)

#### Model comparison

Redo the analysis on actual data. Assume Poisson distribution for the number of page visits. Provide brief interpretation of the analysis results (e.g. WAIC, and 95% credibility interval of coefficients).

```{r}
#include your code and output in the document

#set work path
setwd("D:/seminar/assignment")

#read data and add interaction
web_data <- read.csv("webvisit0.csv") 
web_data$interaction <- web_data$version * web_data$portal

library(rethinking)


# Model formulation
model_bay_web <- map(
  alist(
    pages ~ dpois(lambda),
    log(lambda) <- beta0 + beta1 * version + beta2 * portal + beta3 * interaction,
    beta0 ~ dnorm(0, 10),
    beta1 ~ dnorm(0, 10),
    beta2 ~ dnorm(0, 10),
    beta3 ~ dnorm(0, 10)
  ),
  data = web_data,
)

precis(model_bay_web, prob = .95)

waic <- WAIC(model_bay_web)
waic

# Results:
#       mean   sd  2.5% 97.5%
#beta0  2.98 0.01  2.95  3.00
#beta1 -0.49 0.02 -0.54 -0.45
#beta2  0.52 0.02  0.49  0.56
#beta3  1.01 0.03  0.95  1.06

#     WAIC      lppd  penalty  std_err
#1 6057.62 -3024.845 3.965474 45.57388

#the analysis suggests that the version, portal, and interaction variables have significant effects on the number of page visits. The version and portal variables have opposite effects. The interaction variable shows a stronger positive effect on page visits. The model's WAIC and lppd indicate reasonable fit to the data, 
```



# Part 3 - Multilevel model

## Visual inspection
Use graphics to inspect the distribution of the score, and relationship between session and score. Give a short description of the figure.


```{r}
#include your code and output in the document
```

## Frequentist approach

### Multilevel analysis
Conduct multilevel analysis and calculate 95% confidence intervals thereby assuming a Gaussian distribution for the scores, determine:

* If session has an impact on people score
* If there is significant variance between the participants in their score


```{r}
#include your code and output in the document
```

### Report section for a scientific publication
Write a small section for a scientific publication, in which you report the results of the analyses, and explain the conclusions that can be drawn.

## Bayesian approach
For the Bayesian analyses, use the rethinking and/or BayesianFirstAid library

### Model description

Describe the mathematical model fitted on the most extensive model. (hint, look at the mark down file of the lectures to see example on formulate mathematical models in markdown).  Assume a Gaussian distribution for the scores. Justify the priors.

### Model comparison

Compare models with with increasing complexity. 

```{r}
#include your code and output in the document
```

### Estimates examination

Examine the estimate of parameters of the model with best fit, and provide a brief interpretation.


```{r}
#include your code and output in the document
```

